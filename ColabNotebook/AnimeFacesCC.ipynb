{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N57IxDlosCyk"
      },
      "outputs": [],
      "source": [
        "#@title Generate An Anime Face\n",
        "#@markdown just run the cell the and once the button and slider appear choose your image size\n",
        "#@markdown and click the button to generate a face :)\n",
        "\n",
        "#@markdown the slider is used to resize the image for exmaple 64 is 64x64 resolution\n",
        "#@markdown you will get a plot of fixed size (to show the image if low size the it will be pixlated)\n",
        "\n",
        "#@markdown and another shown in that pixel density which you can save\n",
        "\n",
        "import tensorflow as tf\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import cv2\n",
        "\n",
        "#download and load the saved model\n",
        "url = \"https://drive.google.com/uc?id=1J6AKXvYQMBKTgyw00ROAlFc5KGzsfEL2\"\n",
        "output = 'model.keras'\n",
        "gdown.download(url, output, quiet=False)\n",
        "model = tf.keras.models.load_model('/content/model.keras')\n",
        "seed_size =128 # size of the random noise vector used as input to the generator network\n",
        "\n",
        "\n",
        "#function to genrate output and display\n",
        "# Function to generate output and display it in the J Notebook\n",
        "# This function takes two parameters - the size of the output image and the output widget\n",
        "def generate_face(size, output):\n",
        "\n",
        "    # Clear the output widget to show the new output\n",
        "    with output:\n",
        "        clear_output()\n",
        "\n",
        "    noise = tf.random.normal([1,seed_size]) #generate random noise between 1 and seed_size\n",
        "    generated_images = model(noise) #generate image from noise\n",
        "    generated_images = tf.image.resize(generated_images, (size, size)) # resize the image to the desired size\n",
        "\n",
        "    # Display the generated image\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    # Scale the pixel values of the generated image to the range [0, 1] and display it\n",
        "    ax.imshow((generated_images[0,:,:,:]*0.5+0.5))\n",
        "    ax.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    face_image = (generated_images[0,:,:,:]*0.5+0.5).numpy()\n",
        "    face_image = cv2.resize(face_image, (size, size)) # Resize the image to the desired size using OpenCV library\n",
        "\n",
        "    # Convert to integer array\n",
        "    face_image = (face_image * 255).astype(np.uint8)\n",
        "    if face_image.shape[2] == 1:\n",
        "        face_image = np.squeeze(face_image, axis=2)\n",
        "\n",
        "    return face_image # Return the generated image as an integer array\n",
        "\n",
        "#Create UI elements\n",
        "button_generate = widgets.Button(description=\"Generate Face\")\n",
        "output_image = widgets.Output() # Widget to display the generated image\n",
        "output_message = widgets.Output() # Widget to display messages to the user\n",
        "# Slider widget to adjust the size of the generated image\n",
        "size_slider = widgets.IntSlider(description='Image Size', min=64, max=1024, step=64, value=64)\n",
        "\n",
        "# Define event handlers\n",
        "def on_generate_clicked(b):\n",
        "    # Clear the output message widget and print a message\n",
        "    with output_message:\n",
        "        output_message.clear_output()\n",
        "        print(\"Generating face...\")\n",
        "\n",
        "    #Clear the output image widget and generate and display a face image\n",
        "    with output_image:\n",
        "        output_image.clear_output()\n",
        "        face_image = generate_face(size_slider.value, output_image)\n",
        "        pil_image = Image.fromarray(face_image)\n",
        "        buffered = BytesIO()\n",
        "        pil_image.save(buffered, format=\"JPEG\")\n",
        "        img_widget = widgets.Image(value=buffered.getvalue())\n",
        "        display(img_widget)\n",
        "\n",
        "\n",
        "# Assign event handlers to UI elements\n",
        "button_generate.on_click(on_generate_clicked)\n",
        "\n",
        "# Display the UI\n",
        "display(widgets.VBox([button_generate, size_slider]), output_message, output_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV05SLuCYFod"
      },
      "source": [
        "## Bellow are the cells with the code used for tranning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-cn3e9-TsUx",
        "outputId": "375f660d-a27a-44c2-beed-90de8fa83424"
      },
      "outputs": [],
      "source": [
        "#@title GPU INFO\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cKLyZ55V5zlt"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "from google.colab import output\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense\n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import LeakyReLU , ReLU, PReLU\n",
        "from tensorflow.keras.layers import Conv2D ,Conv2DTranspose\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import urllib.request\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q1JNmkzD5ep7"
      },
      "outputs": [],
      "source": [
        "#@title  Data (Kaggle)\n",
        "#@markdown \"Need a kaggle.jason i.e. your API token uploded in the colab notebook instance to download the data from kaggle\"\n",
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download soumikrakshit/anime-faces\n",
        "!unzip anime-faces.zip\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XiF3OZW_QO_y"
      },
      "outputs": [],
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_7bP2Kok6BSA"
      },
      "outputs": [],
      "source": [
        "#@title Reading and Preprossing / Normalizing data\n",
        "data_path = \"/content/data/data\" #@param   default_path=\"/content/data/data\"\n",
        "batch_size =  125 #@param {type:\"number\"}\n",
        "img_size = 64 #@param {type:\"number\"}\n",
        "colour_mode = 'rgb'\n",
        "\n",
        "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    label_mode=None,\n",
        "    color_mode=colour_mode,\n",
        "    batch_size=batch_size,\n",
        "    image_size=(img_size, img_size),\n",
        "    shuffle=True\n",
        ")\n",
        "#to normalizes each pixel value in the images to be between -1 and 1 for traning\n",
        "train_set = train_set.map(lambda x: ((x/127.5)-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XYRT1gZo7P9v"
      },
      "outputs": [],
      "source": [
        "#@title Show_images   training set\n",
        "ncols = 10 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "nrows = 10 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "def plot_images(ds, nrows=nrows, ncols=ncols):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    ds = ds.unbatch().take(nrows*ncols)\n",
        "    index = 1\n",
        "    for image in ds:\n",
        "        plt.subplot(nrows, ncols, index)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        index += 1\n",
        "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "    plt.show()\n",
        "\n",
        "plot_images(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LThEorMS73_E"
      },
      "outputs": [],
      "source": [
        "#@title strides\n",
        "n_stride = 2 #stride value for convolution layer\n",
        "#n_layers_stride = 2\n",
        "\n",
        "init = tf.keras.initializers.RandomNormal(stddev=0.02)  # initializer for the weights of the layers. with standard deviation of 0.02\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cZZtF0CX76Wx"
      },
      "outputs": [],
      "source": [
        "#@title Generator\n",
        "\n",
        "#function to build the generator\n",
        "def build_generator(seed_size):\n",
        "    model = Sequential()\n",
        "    #first layer dense\n",
        "    model.add(Dense(4*4*1024,kernel_initializer=init,input_dim=seed_size))\n",
        "    model.add(BatchNormalization()) #to stabilize and speed up training.\n",
        "    model.add(ReLU())\n",
        "    model.add(Reshape((4,4,1024)))\n",
        "    #OP_shape = 4,4,1024\n",
        "\n",
        "    #3 convolutional layers,\n",
        "    model.add(Conv2DTranspose(512,kernel_size=5,strides=n_stride,padding='same',use_bias=False,kernel_initializer=init)) #input size = output size\n",
        "    model.add(BatchNormalization()) #to stabilize and speed up training.\n",
        "    model.add(ReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(256,kernel_size=5,strides=n_stride,padding='same',use_bias=False,kernel_initializer=init)) #input size = output size\n",
        "    model.add(BatchNormalization()) #to stabilize and speed up training.\n",
        "    model.add(ReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(128,kernel_size=3,strides=n_stride,padding='same',use_bias=False,kernel_initializer=init)) #input size = output size\n",
        "    model.add(BatchNormalization()) #to stabilize and speed up training.\n",
        "    model.add(ReLU())\n",
        "\n",
        "    #final convolutional layer with tanh\n",
        "    model.add(Conv2DTranspose(3,kernel_size=3,strides=n_stride,padding='same',use_bias=False,kernel_initializer=init)) #input size = output size\n",
        "    model.add(Activation('tanh'))\n",
        "    #OP_shape =  64,64,3\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NDV9xXIz78BH"
      },
      "outputs": [],
      "source": [
        "#@title Discriminator\n",
        "\n",
        "#function to build the discriminator\n",
        "def build_discriminator(image_length,image_channels):\n",
        "\n",
        "    model = Sequential()\n",
        "    #first Conv layer input size = 64,64,3\n",
        "    #input size = output size\n",
        "    model.add(Conv2D(64,kernel_size=3,strides=n_stride,padding='same',use_bias=False,input_shape=(image_length,image_length,image_channels),kernel_initializer=init)) #input size = output size\n",
        "    model.add(LeakyReLU(alpha=0.2)) #LeakyReLU with a negative slope of 0.2, which helps to prevent the vanishing gradient problem during training.\n",
        "\n",
        "    # second Conv layer\n",
        "    model.add(Conv2D(128,kernel_size=3,strides=n_stride,padding='same',use_bias=False,kernel_initializer=init)) #input size = output size\n",
        "    model.add(BatchNormalization()) #to stabilize and speed up training.\n",
        "    model.add(LeakyReLU(alpha=0.2)) #LeakyReLU with a negative slope of 0.2, which helps to prevent the vanishing gradient problem during training.\n",
        "\n",
        "     # third Conv layer\n",
        "    model.add(Conv2D(256,kernel_size=5,strides=n_stride,padding='same',use_bias=False,kernel_initializer=init)) #input size = output size\n",
        "    model.add(BatchNormalization()) #to stabilize and speed up training.\n",
        "    model.add(LeakyReLU(alpha=0.2)) #LeakyReLU with a negative slope of 0.2, which helps to prevent the vanishing gradient problem during training.\n",
        "\n",
        "     # fourth Conv layer\n",
        "    model.add(Conv2D(512,kernel_size=5,strides=n_stride,padding='same',use_bias=False,kernel_initializer=init)) #input size = output size\n",
        "    model.add(BatchNormalization()) #to stabilize and speed up training.\n",
        "    model.add(LeakyReLU(alpha=0.2)) #LeakyReLU with a negative slope of 0.2, which helps to prevent the vanishing gradient problem during training.\n",
        "\n",
        "    #final Conv Layer\n",
        "    model.add(Conv2D(1,kernel_size=4,strides=1,padding='valid',use_bias=False,kernel_initializer=init))  #output size < the input size.\n",
        "    model.add(Flatten()) #flattens the output of the previous layer into a 1D tensor\n",
        "    model.add(Activation('sigmoid')) #maps the output to a probability value between 0 and 1.\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S8GjKZYT8iAO"
      },
      "outputs": [],
      "source": [
        "#@title GAN\n",
        "class GAN(keras.Model):\n",
        "\n",
        "\n",
        "    def __init__(self,seed_size,image_length,image_channels,**kwargs):\n",
        "\n",
        "        super(GAN,self).__init__(**kwargs)\n",
        "        # Initialize the generator and discriminator\n",
        "        self.generator = build_generator(seed_size)\n",
        "        self.discriminator = build_discriminator(image_length,image_channels)\n",
        "        self.seed_size = seed_size\n",
        "\n",
        "\n",
        "    def generator_loss(self,fake_output):\n",
        "      # The generator loss  = the binary cross-entropy loss between the fake output and a tensor of a fake outputs\n",
        "        return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "\n",
        "    def discriminator_loss(self,real_output, fake_output,smooth=0.1):\n",
        "        real_loss = cross_entropy(tf.ones_like(real_output)*(1-smooth), real_output)  #binary cross-entropy loss between the real output and a tensor of real output ,\n",
        "        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) #the binary cross-entropy loss between the fake output and a tensor of zeros\n",
        "        total_loss = real_loss + fake_loss\n",
        "        return total_loss\n",
        "\n",
        "    def compile(self,generator_optimizer,discriminator_optimizer):\n",
        "       # Compile the GAN model with the given optimizers for the generator and discriminator\n",
        "        super(GAN, self).compile()\n",
        "        self.generator_optimizer = generator_optimizer\n",
        "        self.discriminator_optimizer = discriminator_optimizer\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self,data):\n",
        "        # Get the batch size of the input data and generate a random seed of that size\n",
        "        batch_size = tf.shape(data)[0]\n",
        "        seed = tf.random.normal(shape=(batch_size,self.seed_size))\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            generated_image = self.generator(seed, training = True)   # Generate a batch of fake images using the generator network\n",
        "\n",
        "            # Get the discriminator outputs for the real and fake images\n",
        "            real_output = self.discriminator(data,training = True)\n",
        "            fake_output = self.discriminator(generated_image,training = True)\n",
        "\n",
        "            # Calculate the generator and discriminator losses\n",
        "            gen_loss = self.generator_loss(fake_output)\n",
        "            disc_loss = self.discriminator_loss(real_output,fake_output)\n",
        "\n",
        "            # Calculate the gradients of the generator and discriminator losses with respect to the trainable variables of their respective networks\n",
        "            generator_grad = gen_tape.gradient(gen_loss,self.generator.trainable_variables)\n",
        "            discriminator_grad = disc_tape.gradient(disc_loss,self.discriminator.trainable_variables)\n",
        "\n",
        "            # Apply the gradients to update the trainable variables of the generator and discriminator networks\n",
        "            self.generator_optimizer.apply_gradients(zip(generator_grad,self.generator.trainable_variables))\n",
        "            self.discriminator_optimizer.apply_gradients(zip(discriminator_grad,self.discriminator.trainable_variables))\n",
        "\n",
        "        # Return generator and discriminator losses for the current batch\n",
        "        return {\n",
        "            \"generator loss\": gen_loss,\n",
        "            \"discriminator_loss\": disc_loss\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vYAYBmHRCF8j"
      },
      "outputs": [],
      "source": [
        "#@title Callbacks\n",
        "class callbacks(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self,noise,margin,num_rows,num_cols,**kwargs):\n",
        "        super(keras.callbacks.Callback,self).__init__(**kwargs)\n",
        "        # Initialize callback variables\n",
        "        self.noise = noise         #Noise input used to generate images\n",
        "        self.margin = margin       #Margin between generated images\n",
        "        self.num_rows = num_rows   #Number of rows of generated images to display\n",
        "        self.num_cols = num_cols   #Number of columns of generated images to display\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Create a blank image array to store the generated images\n",
        "        image_array = np.full((\n",
        "            self.margin + (self.num_rows * (64 + self.margin)),    #Height of the image array\n",
        "            self.margin + (self.num_cols * (64 + self.margin)),    # Width of the image array\n",
        "            3), # 3 color channels for the image array\n",
        "            255, dtype=np.uint8)  #Set the background color to white\n",
        "\n",
        "        generated_images = self.model.generator.predict(self.noise) # Generate images using the generator model\n",
        "        generated_images = 0.5 * generated_images + 0.5  # Scale the pixel values of the generated images to be between 0 and 1\n",
        "\n",
        "        image_count = 0   # Counter for keeping track of the current image being generated\n",
        "        # Iterate through the rows and columns of the image array and add the generated images\n",
        "        for row in range(self.num_rows):\n",
        "            for col in range(self.num_cols):\n",
        "                # Calculate the position of the current image in the image array\n",
        "                r = row * (64 + 16) + self.margin\n",
        "                c = col * (64 + 16) + self.margin\n",
        "                image_array[r:r + 64, c:c + 64] = generated_images[image_count] * 255  # Add the current generated image to the image array\n",
        "                image_count += 1   # Increment image count\n",
        "\n",
        "        output_path = 'Traning_images' #path where the images will be saved\n",
        "        #if the path doesn't exist then make that folder\n",
        "        if not os.path.exists(output_path):\n",
        "            os.makedirs(output_path)\n",
        "        # Save the generated image array as a PNG file\n",
        "        filename = os.path.join(output_path, f\"train-{epoch+1}.png\")\n",
        "        im = Image.fromarray(image_array)\n",
        "        im.show(filename)  #display traning  image for that epoch\n",
        "        #if epoch % 20 == 0:\n",
        "          #im.save(filename)\n",
        "\n",
        "\n",
        "class checkpoint_callback(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self,**kwargs):\n",
        "        super(keras.callbacks.Callback, self).__init__(**kwargs)\n",
        "    #uncomment two lines below if you want to save the training weights after every epoch\n",
        "    #def on_epoch_end(self, epoch, logs=None):\n",
        "        #self.model.generator.save_weights(\"/content/drive/MyDrive/data/generator_weights.h5\")\n",
        "        #self.model.discriminator.save_weights(\"/content/drive/MyDrive/data/discriminator_weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoJ5U-xgLwYn"
      },
      "source": [
        "Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IEpJI5sh8lA5"
      },
      "outputs": [],
      "source": [
        "#@title parameters\n",
        "image_length = 64  #@param   # image length in pixels\n",
        "image_channels = 3 #@param # number of color channels (3 = RGB)(1 = greyscale but have to do appropriate changes to gena nd disc to compansate)\n",
        "batch_size = 128 #@param\n",
        "seed_size = 128 #@param\n",
        "\n",
        "NUM_ROWS = 4  #the number of rows in the grid of images\n",
        "NUM_COLS = 7 #the number of columns in the grid of images.\n",
        "MARGIN = 16 #the number of pixels of margin between each image in the grid.\n",
        "\n",
        "fixed_seed = tf.random.normal(shape=(NUM_ROWS * NUM_COLS, seed_size)) #fixed random seed for generating example images\n",
        "GEN_LR = 0.0004 #@param {type:\"number\"} #Generator Learning Rate\n",
        "DISC_LR = 0.0005 #@param {type:\"number\"} #Discriminator Learning Rate\n",
        "\n",
        "N_EPOCS = 1 #@param {type:\"number\"} #number of training epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xGCtXpiO8rJ-"
      },
      "outputs": [],
      "source": [
        "#@title initlize Optimizers and Compile GAN\n",
        "generator_optimizer = Adam(learning_rate=GEN_LR,beta_1=0.5) #Initialize optimizer for the generator using Adam with a learning rate of GEN_LR and beta1 of 0.5.\n",
        "discriminator_optimizer = Adam(learning_rate=DISC_LR,beta_1=0.5) #Initialize optimizer for the discriminator using Adam with a learning rate of DISC_LR and beta1 of 0.5.\n",
        "\n",
        "gan = GAN(seed_size,image_length,image_channels) #Initialize the GAN model\n",
        "gan.compile(generator_optimizer,discriminator_optimizer) #Compile the GAN model with given optimizers for the generator and discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-8mldj9DtxjC"
      },
      "outputs": [],
      "source": [
        "#@title load model weights from URL\n",
        "\n",
        "# set the URL of the generator and discriminator weights file\n",
        "url_gen = \"https://drive.google.com/uc?export=download&id=1YyhIjmY2OCJgWg0uz6FlQ5yYZu_Ql7S5\" #@param\n",
        "url_disc = \"https://drive.google.com/uc?export=download&id=1pl5fhB6BynXu6VVObU0V5jxiEa2uZkJv\" #@param\n",
        "\n",
        "# set the filename of the generator and discriminator weights files\n",
        "g = \"generator_weights.h5\"\n",
        "d = \"discriminator_weights.h5\"\n",
        "\n",
        "# download the generator weights file from the URL and save it to the filename\n",
        "urllib.request.urlretrieve(url_gen, g)\n",
        "\n",
        "# load the weights from the generator weights file into the GAN model's generator\n",
        "gan.generator.load_weights(g)\n",
        "\n",
        "# download the discriminator weights file from the URL and save it to the filename\n",
        "urllib.request.urlretrieve(url_disc, d)\n",
        "\n",
        "# load the weights from the discriminator weights file into the GAN model's discriminator\n",
        "gan.discriminator.load_weights(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_WST7DaL7Ei"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3HlqE_cCDDYJ"
      },
      "outputs": [],
      "source": [
        "#@title load weights from G_drive\n",
        "\n",
        "#gan.generator.load_weights(\"/content/generator_weights.h5\")\n",
        "#gan.discriminator.load_weights(\"/content/discriminator_weights.h5\")\n",
        "#gan.generator.load_weights(\"/content/drive/MyDrive/data/generator_weights.h5\")\n",
        "#gan.discriminator.load_weights(\"/content/drive/MyDrive/data/discriminator_weights.h5\")\n",
        "\n",
        "#op = gan.fit(train_set,epochs=N_EPOCS,batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dbl8jha1q-NF"
      },
      "outputs": [],
      "source": [
        "#@title Traning\n",
        "#train GAN\n",
        "\n",
        "#comment out call backs if you dont want to call call back class\n",
        "op = gan.fit(train_set,epochs=N_EPOCS,batch_size=batch_size\n",
        "             ,callbacks=[callbacks(noise=fixed_seed,num_rows=NUM_ROWS,num_cols=NUM_COLS,margin=MARGIN), checkpoint_callback()]\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4ee7ZSVGGshT"
      },
      "outputs": [],
      "source": [
        "#@title test genertating many outputs\n",
        "#@markdown this cell genrates faces using generator\n",
        "#@markdown set the number of images to be generated using a slider and run the cell to get the output\n",
        "how_many_outputs_to_gen = 64 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "def generate_faces():\n",
        "\n",
        "    noise = tf.random.normal([how_many_outputs_to_gen,seed_size]) # generate random noise\n",
        "    generated_images = gan.generator(noise) # use the generator to generate images from the random noise\n",
        "\n",
        "    fig = plt.figure(figsize=(12,12))  #create a figure to display the generated images\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(12,12,i+1) #add a subplot for each generated image\n",
        "        #display the generated image and adjust the range of pixel values to be between 0 and 1\n",
        "        plt.imshow((generated_images[i,:,:,:]*0.5+0.5))\n",
        "        plt.axis(\"off\")  #turn off the axes for the subplot\n",
        "    plt.show() #show the figure with the generated images\n",
        "\n",
        "generate_faces() #call the function to generate random faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "czcAQj6ryIyq"
      },
      "outputs": [],
      "source": [
        "#@title Build just the genertor and load weights\n",
        "\n",
        "#@markdown must run the code wher the imports, Strides and generator sections above before running this\n",
        "\n",
        "seed_size = 128\n",
        "model = build_generator(seed_size)\n",
        "url_g = \"https://drive.google.com/uc?export=download&id=1YyhIjmY2OCJgWg0uz6FlQ5yYZu_Ql7S5\" #@param\n",
        "g = \"generator_weights.h5\"\n",
        "urllib.request.urlretrieve(url_g, g)\n",
        "model.load_weights(g)\n",
        "\n",
        "def generate_face():\n",
        "\n",
        "    noise = tf.random.normal([1,seed_size])\n",
        "    generated_images = model(noise)\n",
        "\n",
        "    fig = plt.figure(figsize=(12,12))\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(2,2,i+1)\n",
        "        plt.imshow((generated_images[i,:,:,:]*0.5+0.5))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    plt.savefig(\"face.png\")\n",
        "\n",
        "#model.save('/content/drive/MyDrive/Saved_models/AF.keras')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wh222_wZyOhm"
      },
      "outputs": [],
      "source": [
        "#@title GET _Output\n",
        "generate_face()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AV05SLuCYFod"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
